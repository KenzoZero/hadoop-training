# Launch the interactive spark shell
$ /opt/mapr/spark/spark-1.6.1/bin/spark-shell --master local[2]

# Create a SQL context
> val sqlContext = new org.apache.spark.sql.SQLContext(sc)
> import sqlContext.implicits._

# Define the Auctions case class
> case class Auctions(aucid:String, bid:Float, bidtime:Float, bidder:String, bidrate:Int, openbid:Float, price:Float, itemtype:String, dtl:Int)

# Load the data
val inputRDD = sc.textFile("/user/user01/data/auctiondata.csv").map(_.split(","))

# Map the input RDD to the case class
val auctionsRDD = inputRDD.map(a => Auctions(a(0), a(1).toFloat, a(2).toFloat, a(3), a(4).toInt, (5).toFloat, a(6).toFloat, a(7), a(8).toInt))

# Convert RDD to a DataFrame
val auctionsDF = auctionsRDD.toDF()

# Register as a temporary table with the same name
auctionsDF.registerTempTable("auctionsDF")

# Check the data in the DataFrame
auctionsDF.show()

# See the schema of the DataFrame
auctionsDF.printSchema()

# Total number of bids
val totalbids = auctionsDF.count()

# Number of distinct auctions
auctionsDF.select("aucid").distinct.count()

# Number of distinct item types
auctionsDF.select("itemtype").distinct.count()

# Count of bids per auction and item type
auctionsDF.groupBy("itemtype", "aucid").count().show()

# For each auction item and item type, get the min, max and average number of bids
auctionsDF.groupBy("itemtype", "aucid").count.agg(min("count"), avg("count"), max("count")).show()

# For each auction item and item type, get the min, max, and average bid
auctionsDF.groupBy("itemtype", "aucid").agg(min("bid"), max("bid"), avg("bid")).show()

# Return the count of all auctions with final price greater than 200
auctionsDF.filter(auctionsDF("price") > 200).count()

# Select just xbox auctions
val xboxes = sqlContext.sql("SELECT aucid, itemtype, bid, price, openbid FROM auctionsDF WHERE itemtype='xbox'")

# Compute basic statistics on price across all auctions on xboxes
xboxes.describe("price").show
